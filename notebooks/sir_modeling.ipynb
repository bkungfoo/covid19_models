{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing SIR Models for COVID-19\n",
    "\n",
    "Objectives: Look at the rate of COVID-19 growth by different regions and estimate the SIR curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from scipy import optimize\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "# Use parent directory as a base code directory.\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from modeling import dataproc, optimizer, sir_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Covid-19 and Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datastore = dataproc.DataStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "Functions that are called to plot the curve, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sir_model(r, i, s, sim_days, plot_days, df, metric, sampling_rate, name):\n",
    "    \"\"\"Plot the model death rates and total deaths vs actual data.\n",
    "    \n",
    "    Args:\n",
    "        r: Array holding daily recovered population values from SIR model\n",
    "        i: Array holding daily infected population values from SIR model\n",
    "        s: Array holding daily susceptible population values from SIR model\n",
    "        total_model_days: Total number of modeled days to plot\n",
    "        df: Dataframe holding metric values.\n",
    "        metric: The type of metric to plot ('Cases' or 'Deaths')\n",
    "        sampling_rate: Number of samples per day used to simulate the model.\n",
    "        name: A name to attach to the plot.\n",
    "    \"\"\"\n",
    "    start_time = df['Date'].min().timestamp()\n",
    "    step_size = 24 * 60 * 60 / sampling_rate\n",
    "    plot_end_time = start_time + plot_days * 24 * 60 * 60 \n",
    "    sim_end_time = start_time + sim_days * 24 * 60 * 60 \n",
    "    plot_timestamps = np.arange(start_time, plot_end_time, step_size)\n",
    "    sim_timestamps = np.arange(start_time, sim_end_time, step_size)\n",
    "    plot_dates = [datetime.utcfromtimestamp(x) for x in plot_timestamps]\n",
    "    sim_dates = [datetime.utcfromtimestamp(x) for x in sim_timestamps]\n",
    "    print('peak infection rate date', sim_dates[np.argmax(i * s)])\n",
    "    # Plot peak infection\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    ax.ticklabel_format(style='plain')\n",
    "    ax.plot(plot_dates[:-sampling_rate],\n",
    "            (r[sampling_rate:len(plot_dates)] + i[sampling_rate:len(plot_dates)]\n",
    "             - r[:len(plot_dates)-sampling_rate] - i[:len(plot_dates)-sampling_rate]),\n",
    "            c='g',\n",
    "            label='model ' + metric + ' rate',\n",
    "            linewidth=4)\n",
    "    ax.plot(df['Date'].to_list()[:-1],\n",
    "            (df[metric] - df[metric].shift())[1:], label='actual ' + metric + ' rate', c='r', linewidth=4)\n",
    "    ax.set_title('SIR model for ' + name)\n",
    "    ax.set_xlabel('Number of days')\n",
    "    ax.set_ylabel('Number of individuals')\n",
    "    plt.legend()\n",
    "    plt.plot()\n",
    "    \n",
    "    # Plot recovery\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    ax.ticklabel_format(style='plain')\n",
    "    ax.plot(plot_dates, r[:len(plot_dates)] + i[:len(plot_dates)], c='g',\n",
    "            label='model ' + metric, linewidth=4)\n",
    "    ax.plot(df['Date'].to_list(), df[metric], label='actual ' + metric, c='r', linewidth=4)\n",
    "    ax.set_title('SIR model for ' + name)\n",
    "    ax.set_xlabel('Number of days')\n",
    "    ax.set_ylabel('Number of individuals')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATION_DAYS = 360 # Total number of days to simulate when plotting forecast model.\n",
    "SAMPLING_RATE = 10 # Modeling time samples per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking up FIPS for states and counties to model\n",
    "\n",
    "Use the following query to obtain state and county FIPS of interest. This can be used in the below AREA_OF_INTEREST assignment, or you can also use one of the folowing regions of interest below\n",
    "\n",
    "### Regions of interest\n",
    "\n",
    "Some interesting areas (Name, State FIPS, County FIPS) below. Copy one of the values in the bullet points into AREA_OF_INTEREST below.\n",
    "* ('US', 'NYC', 36, [5, 47, 61, 81, 85])\n",
    "* ('US', 'New Orleans', 22, [51, 71, 75, 87, 89, 95, 103, 105])\n",
    "* ('US', 'Detroit', 26, [87, 93, 99, 125, 147, 163])\n",
    "* ('US', 'Bay Area, CA', 6, [1, 13, 41, 55, 75, 81, 85, 95, 97])\n",
    "* ('US', 'Greater LA Area, CA', 6, [37, 59, 65, 71, 111])\n",
    "* ('US', 'Chicago', 17, [31, 37, 43, 63, 89, 91, 93, 111, 197])\n",
    "* ('US', 'Houston', 48, [15, 39, 71, 157, 167, 201, 291, 339, 473])\n",
    "* ('US', 'Austin', 48, [209, 453, 491])\n",
    "* ('US', 'Miami', 12, [11, 86, 99])\n",
    "* ('US', 'Tampa-St. Petersburg', 12, [53, 57, 101, 103])\n",
    "* ('US', 'Tulsa, Oklahoma', 40, [113, 131, 143, 145])\n",
    "* ('US', 'Lake Tahoe', [(6, [17, 61]), (32, [5, 31, 510]),])\n",
    "\n",
    "States:\n",
    "* ('US', 'Alabama', 1 , [])\n",
    "* ('US', 'Arizona', 4, [])\n",
    "* ('US', 'California', 6, [])\n",
    "* ('US', 'Connecticut', 9, [])\n",
    "* ('US', 'Florida', 12, [])\n",
    "* ('US', 'Georgia', 13, [])\n",
    "* ('US', 'Illinois', 17, [])\n",
    "* ('US', 'Iowa', 19, [])\n",
    "* ('US', 'Louisiana', 22, [])\n",
    "* ('US', 'Massachusetts', 25, [])\n",
    "* ('US', 'Michigan', 26, [])\n",
    "* ('US', 'Mississippi', 28, [])\n",
    "* ('US', 'New Jersey', 34, [])\n",
    "* ('US', 'New Mexico', 35, [])\n",
    "* ('US', 'New York', 36, [])\n",
    "* ('US', 'Ohio', 39, [])\n",
    "* ('US', 'Pennsylvania', 42, [])\n",
    "* ('US', 'Texas', 48, [])\n",
    "* ('US', 'Utah', 49, []) \n",
    "* ('US', 'Washington', 53, [])\n",
    "\n",
    "Top 10 total death states:\n",
    "\n",
    "('US', 'New York', 36, []),\n",
    "('US', 'New Jersey', 34, []),\n",
    "('US', 'Massachusetts', 25, []),\n",
    "('US', 'Illinois', 17, []),\n",
    "('US', 'Pennsylvania', 42, []),\n",
    "('US', 'Michigan', 26, []),\n",
    "('US', 'California', 6, []),\n",
    "('US', 'Connecticut', 9, []),\n",
    "('US', 'Florida', 12, []),\n",
    "('US', 'Louisiana', 22, []),\n",
    "\n",
    "If Country is not US, this will fetch a country's total stats:\n",
    "* ('Italy', 'Italy')\n",
    "* ('Spain', 'Spain')\n",
    "* ('United Kingdom', 'United Kingdom')\n",
    "* ('US', 'US')\n",
    "* ('Germany', 'Germany')\n",
    "* ('India', 'India')\n",
    "* ('Brazil', 'Brazil')\n",
    "* ('Mexico', 'Mexico')\n",
    "* ('Peru', 'Peru')\n",
    "* ('Canada', 'Canada')\n",
    "* ('Pakistan', 'Pakistan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on looking up state and county FIPS\n",
    "\n",
    "lookup_df = datastore.county_census_df[(datastore.county_census_df.STNAME == 'California')\n",
    "                            & (datastore.county_census_df.CTYNAME.isin([\n",
    "                                 'Santa Clara County',\n",
    "#                                  'Madera County',\n",
    "#                                 'Tuolumne County'\n",
    "# #                                  'Tulsa County',\n",
    "#                                  'Wagoner County',\n",
    "                            ]))\n",
    "]\n",
    "print('state fips', lookup_df['STATE'].iloc[0])\n",
    "print('county fips', lookup_df['COUNTY'].tolist())\n",
    "lookup_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AREA_OF_INTEREST = ('US', 'All except NY/NJ', [(i, []) for i in range(50) if i not in [34, 36]])\n",
    "#AREA_OF_INTEREST = ('US', 'All except NY/NJ', [(i, []) for i in range(50)])\n",
    "#AREA_OF_INTEREST = ('US', 'Lake Tahoe', [(6, [17, 61]), (32, [5, 31, 510]),])\n",
    "MODEL_FIT_LAST_DATE = datetime(2021, 2, 25)  # Fit model to data before this date, reserving later dates as holdout.\n",
    "MODEL_FIT_FIRST_DATE = MODEL_FIT_LAST_DATE - timedelta(120)#'2020-04-25'\n",
    "METRIC = 'Deaths'\n",
    "pop_frac = 0.006\n",
    "# METRIC = 'Cases'\n",
    "# pop_frac = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(AREA_OF_INTEREST) <= 2:\n",
    "    area_df, population = datastore.get_time_series_for_area(AREA_OF_INTEREST[0])\n",
    "else:\n",
    "    area_df, population = datastore.get_time_series_for_area(\n",
    "        AREA_OF_INTEREST[0], AREA_OF_INTEREST[2])\n",
    "    \n",
    "print('Total population', population)\n",
    "area_df = area_df[['Date', METRIC]]\n",
    "train_area_df = area_df[\n",
    "    (area_df.Date >= MODEL_FIT_FIRST_DATE) & \n",
    "    (area_df.Date <= MODEL_FIT_LAST_DATE)][['Date', METRIC]]\n",
    "train_area_df = train_area_df[train_area_df[METRIC] > 0]\n",
    "train_area_df = train_area_df.sort_values(by=['Date']).reset_index(drop=True)\n",
    "# Validate selection through plot and inspection\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_area_df['Date'], train_area_df[METRIC])\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_area_df['Date'], (train_area_df[['Date', METRIC]]-train_area_df[['Date', METRIC]].shift(1))[METRIC])\n",
    "train_area_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detrend by day of week\n",
    "DAYS_OF_THE_WEEK = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "area_df['dow'] = area_df['Date'].dt.dayofweek\n",
    "area_df[METRIC + ' diff'] = area_df[METRIC] - area_df[METRIC].shift(1)\n",
    "area_df[METRIC + ' Squared diff'] = area_df[METRIC + ' diff'] * area_df[METRIC + ' diff']\n",
    "dow_sum = None\n",
    "weeks = 4\n",
    "\n",
    "weekly_sum = 0\n",
    "weekly_sum_squared = 0\n",
    "for days_before in range(weeks*7-1, weeks*7+6): #7 days shift\n",
    "    for week in range(weeks):\n",
    "        start_date = area_df['Date'].max() + timedelta(-days_before+week*7)\n",
    "        norm_dow = area_df[\n",
    "            (area_df['Date'] >= start_date) &\n",
    "            (area_df['Date'] <= start_date + timedelta(6))\n",
    "        ][['Date', METRIC, METRIC + ' diff', METRIC + ' Squared diff', 'dow']].groupby('dow').sum()\n",
    "        dow_mean = norm_dow[METRIC + ' diff'].mean()\n",
    "        norm_dow[METRIC + ' diff'] /= (dow_mean + 1e-4)\n",
    "        norm_dow[METRIC + ' diff'] = np.maximum(norm_dow[METRIC + ' diff'], 1e-4)\n",
    "        norm_dow[METRIC + ' Squared diff'] /= ((dow_mean + 1e-4) ** 2)\n",
    "        norm_dow[METRIC + ' Squared diff'] = np.maximum(norm_dow[METRIC + ' Squared diff'], 1e-8)\n",
    "        weekly_sum += norm_dow[METRIC + ' diff'].sum()\n",
    "        weekly_sum_squared += norm_dow[METRIC + ' Squared diff'].sum()\n",
    "        if dow_sum is None: # 1 week of data\n",
    "            dow_sum = norm_dow\n",
    "        else:\n",
    "            dow_sum += norm_dow\n",
    "            \n",
    "# Total number of samples is weeks * 7, so we take the mean by dividing by weeks * 7\n",
    "death_mean = dow_sum[METRIC + ' diff'] / weeks / 7\n",
    "death_err = np.sqrt(dow_sum[METRIC + ' Squared diff'] / weeks / 7 - death_mean * death_mean) / np.sqrt(weeks)\n",
    "weekly_mean = weekly_sum / weeks / 7 / 7\n",
    "weekly_err = np.sqrt(weekly_sum_squared / weeks / 7 / 7 - weekly_mean * weekly_mean) / np.sqrt(weeks)\n",
    "# Apply Gaussian weighted averaging\n",
    "kf_mean = (death_mean * (1 / death_err ** 2) + weekly_mean * (1 / weekly_err ** 2)\n",
    "                         ) / ((1 / death_err ** 2) + (1 / weekly_err ** 2))\n",
    "kf_err = np.sqrt(1 / ((1 / death_err ** 2) + (1 / weekly_err ** 2)))\n",
    "print(kf_mean)\n",
    "plt.bar(DAYS_OF_THE_WEEK, death_mean, yerr=death_err, capsize=7)\n",
    "plt.title(AREA_OF_INTEREST[1] + ' detrended day-of-week ' + METRIC + ' (' + str(weeks) + ' week average)')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(DAYS_OF_THE_WEEK, kf_mean, yerr=kf_err, capsize=7)\n",
    "plt.title(AREA_OF_INTEREST[1] + ' kalman filtered day-of-week ' + METRIC + ' (' + str(weeks) + ' week average)')\n",
    "plt.show()\n",
    "\n",
    "diff_df = train_area_df-train_area_df.shift(1)\n",
    "diff_df['Date'] = train_area_df['Date']\n",
    "diff_df['dow'] = diff_df['Date'].dt.dayofweek\n",
    "\n",
    "diff_df[METRIC + ' adjusted'] = diff_df[METRIC] / (diff_df['dow'].apply(lambda x: kf_mean[x]) + 1e-8)\n",
    "diff_df[METRIC + ' fully adjusted'] = diff_df[METRIC] / (diff_df['dow'].apply(lambda x: death_mean[x]) + 1e-8)\n",
    "diff_df[METRIC].iloc[0] = 0\n",
    "diff_df[METRIC + ' adjusted'].iloc[0] = 0\n",
    "\n",
    "diff_df[METRIC + ' 7-day mean'] = diff_df[METRIC].rolling(7, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(diff_df['Date'], diff_df[METRIC], label=METRIC)\n",
    "plt.plot(diff_df['Date'], diff_df[METRIC + ' adjusted'], label=METRIC + ' adjusted')\n",
    "plt.plot(diff_df['Date'], diff_df[METRIC + ' fully adjusted'], label=METRIC + ' fully, adjusted')\n",
    "plt.plot(diff_df['Date'], diff_df[METRIC + ' 7-day mean'], label=METRIC + ' 7 day mean', linewidth=4)\n",
    "#plt.xticks(diff_df['Date'], diff_df['dow'], rotation='vertical')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "diff_df[METRIC + ' cumsum'] = diff_df[METRIC].cumsum()\n",
    "diff_df[METRIC + ' adjusted cumsum'] = diff_df[METRIC + ' adjusted'].cumsum()\n",
    "plt.plot(diff_df[METRIC + ' adjusted cumsum'] * diff_df[METRIC + ' cumsum'].iloc[-1] / diff_df[METRIC + ' adjusted cumsum'].iloc[-1],\n",
    "         linewidth=4)\n",
    "plt.plot(diff_df[METRIC + ' cumsum'], linewidth=2)\n",
    "\n",
    "#train_area_df[METRIC + ' adjusted'] = train_area_df[METRIC].iloc[0] + diff_df[METRIC + ' adjusted cumsum'] * diff_df[METRIC + ' cumsum'].iloc[-1] / diff_df[METRIC + ' adjusted cumsum'].iloc[-1]\n",
    "train_area_df[METRIC + ' adjusted'] = train_area_df[METRIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: Get rid of single day outliers by using a window to limit the outlier slope\n",
    "# to the second largest/smallest slope\n",
    "# Rescale to keep the total number of deaths equal.\n",
    "\n",
    "\n",
    "train_data = dataproc.convert_data_to_numpy(train_area_df, METRIC + ' adjusted', smooth=False)\n",
    "plt.yscale('log')\n",
    "plt.plot(train_area_df['Date'], train_area_df[METRIC], linewidth=4, label='raw total ' + METRIC)\n",
    "plt.plot(train_area_df['Date'], train_data, linewidth=4, label='smoothed total ' + METRIC)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_area_df['Date'].iloc[:-1],\n",
    "         train_area_df[METRIC].iloc[1:].to_numpy() - train_area_df[METRIC].iloc[:-1].to_numpy(),\n",
    "         linewidth=4, label='raw ' + METRIC + ' rates')\n",
    "plt.plot(train_area_df['Date'].iloc[:-1],\n",
    "         train_data[1:] - train_data[:-1],\n",
    "         linewidth=4, label='smoothed ' + METRIC + ' rates')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Values to the Model\n",
    "\n",
    "We try to find the best fit of all parameters of the model by minimizing its mean squared error (mse) from actual data points.\n",
    "\n",
    "Note that the simple algorithm used below is randomized and not guaranteed to be optimal, but in practice, seems to converge to a near optimal solution quickly. Also, approaches such as Bayesian optimization, annealing, and other guaranteed optimal techniques take a long time to run per iteration and have occasionally stalled the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reload(sir_model)\n",
    "reload(optimizer)\n",
    "# Reasonable search regions for each parameter\n",
    "recovery_days = 10.0 # This is fairly constant\n",
    "\n",
    "infection_rate_range = [0.001, 0.80]\n",
    "multiplier_range = [0.01, 10.0]\n",
    "frac_infected_range = [0.01, 0.99]\n",
    "\n",
    "best_param, best_value = optimizer.minimize(\n",
    "    train_data, population, recovery_days,\n",
    "    pop_frac, infection_rate_range, multiplier_range, frac_infected_range\n",
    ")\n",
    "print('Param', best_param)\n",
    "print('MSE', best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Validation plot\n",
    "validation_area_df = area_df # TODO: add holdout days\n",
    "validation_area_df = validation_area_df[\n",
    "    (validation_area_df.Date >= MODEL_FIT_FIRST_DATE)\n",
    "    & (validation_area_df.Date <= MODEL_FIT_LAST_DATE)\n",
    "]\n",
    "validation_area_df = validation_area_df[validation_area_df[METRIC] > 0]\n",
    "validation_area_df = validation_area_df.sort_values(by=['Date']).reset_index(drop=True)\n",
    "\n",
    "best_infection_rate = best_param[0]\n",
    "best_multiplier = best_param[1]\n",
    "best_frac_infected = best_param[2]\n",
    "\n",
    "infected = train_data[0] * best_multiplier * best_frac_infected\n",
    "recovered = train_data[0] * best_multiplier * (1 - best_frac_infected)\n",
    "t, s, i, r = sir_model.compute_sir(\n",
    "    SAMPLING_RATE,\n",
    "    SIMULATION_DAYS,\n",
    "    population * pop_frac,\n",
    "    infected,\n",
    "    recovered,\n",
    "    best_infection_rate,\n",
    "    recovery_days\n",
    ")\n",
    "\n",
    "valid_obj = sir_model.create_objective_fn(\n",
    "    validation_area_df[METRIC].to_numpy(), population, sampling_rate=SAMPLING_RATE)\n",
    "\n",
    "validation_mse = valid_obj(pop_frac, best_infection_rate, recovery_days, best_multiplier, best_frac_infected)\n",
    "\n",
    "print('Population fraction susceptible (e.g. would die if infected):', pop_frac)\n",
    "print('Population susceptible (e.g. would die if infected):', pop_frac * population)\n",
    "print('Fraction of infected/recovered population currently infected at start of simulation:', best_frac_infected)\n",
    "print('Final population affected (e.g. dead) since start of simulation:', (s[0] - s[-1]))\n",
    "print('Final population affected (e.g. dead) overall:', (train_area_df[METRIC].iloc[0] + s[0] - s[-1]))\n",
    "print('Current and final herd immunity level:',\n",
    "      train_data[-1] / pop_frac / population,\n",
    "      (population * pop_frac - s[-1]) / pop_frac / population)\n",
    "print('Transmissions per person per day:', best_infection_rate)\n",
    "print('First day estimate multiplier', best_multiplier)\n",
    "print('R0 (initial transmit rate / recovery rate)', best_infection_rate * recovery_days)\n",
    "print('R (current transmit rate / recovery rate)', best_infection_rate * recovery_days * (1 - train_data[-1] / pop_frac / population))\n",
    "print('Training MSE', best_value)\n",
    "print('Validation MSE', validation_mse)\n",
    "print('2 week numbers', np.sum(train_data[-14:] - train_data[-15:-1]), 'fraction')\n",
    "plot_sir_model(r, i, s, SIMULATION_DAYS, 60, validation_area_df, METRIC, SAMPLING_RATE, AREA_OF_INTEREST[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 24 * 60 * 60 / SAMPLING_RATE\n",
    "start_time = validation_area_df['Date'].min().timestamp()\n",
    "sim_end_time = start_time + SIMULATION_DAYS * 86400\n",
    "sim_timestamps = np.arange(start_time, sim_end_time, step_size)\n",
    "sim_dates = [datetime.utcfromtimestamp(x) for x in sim_timestamps]\n",
    "list(zip(sim_dates, r + i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
