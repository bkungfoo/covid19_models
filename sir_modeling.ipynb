{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIR Modeling for COVID-19\n",
    "\n",
    "Objectives: Look at the rate of COVID-19 growth by different regions and estimate the SIR curve.\n",
    "\n",
    "* The recovery rate and overall fatality rate should be fixed.\n",
    "* The infection rate should depend on local population density and social distancing measures (unique to each region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from scipy import optimize\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Covid-19 and Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/us_combined_df.pkl', 'rb') as f:\n",
    "    us_combined_df = pickle.load(f)\n",
    "\n",
    "# Load covid and world population data\n",
    "with open('./data/world_combined_df.pkl', 'rb') as f:\n",
    "    world_combined_df = pickle.load(f)\n",
    "    \n",
    "county_census_df = pd.read_csv('./data/co-est2019-alldata.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "Functions that are called to generate an SIR model, plot the curve, compute objective functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several utility functions for SIR model\n",
    "\n",
    "def compute_sir(sampling_rate, total_days, pop, infected, contacts_per_day, days_to_recover):\n",
    "    \"\"\"Simulates the SIR output over a number of days using small fraction-of-day time steps.\n",
    "    \n",
    "    Args:\n",
    "        sampling_rate: The number of samples per day.\n",
    "        total_days: The total time to simulate.\n",
    "        pop: The population of the area we are simulating.\n",
    "        infected: The starting number of infected individuals.\n",
    "        contacts_per_day: The number of people each infected individual infects per day\n",
    "          at the start of simulation (nearly everyone is susceptible).\n",
    "        days_to_recover: The number of days it takes someone to recover from the disease.\n",
    "        \n",
    "    Returns:\n",
    "        Time indices, and associated susceptible, infected, and recovered populations\n",
    "        \n",
    "    \"\"\"\n",
    "    s = [1.0]\n",
    "    i = [float(infected) / pop]\n",
    "    r = [0.0]\n",
    "    beta = contacts_per_day\n",
    "    gamma = 1.0 / days_to_recover\n",
    "    dt = 1.0 / sampling_rate\n",
    "    for t in np.arange(0, total_days-dt, dt):\n",
    "        prev_s = s[-1]\n",
    "        prev_i = i[-1]\n",
    "        prev_r = r[-1]\n",
    "        # First order modeling\n",
    "        s.append(prev_s - beta * prev_s * prev_i * dt)\n",
    "        i.append(prev_i + (beta * prev_s - gamma) * prev_i * dt)\n",
    "        r.append(prev_r + gamma * prev_i * dt)\n",
    "        \n",
    "    s = np.array(s)\n",
    "    i = np.array(i)\n",
    "    r = np.array(r)\n",
    "    return np.arange(0, total_days, dt), pop * s, pop * i, pop * r\n",
    "\n",
    "\n",
    "def create_mse_objective_fn(deaths, population, sampling_rate, starting_cases):\n",
    "    \"\"\"Create an objective function for Bayesian optimization using MSE of model vs actual data.\n",
    "    \n",
    "    Args:\n",
    "      data: A numpy array with the raw daily data to model.\n",
    "      population: The total population of the area to model.\n",
    "      sampling_rate: The sampling_rate for the model in number of samples per day.\n",
    "    \n",
    "    Returns:\n",
    "      A function that takes tuples for ranges of death_rate, contacts_per_day, and days_to_recover\n",
    "      and returns -mse(model, actual) as a function to maximize.\n",
    "    \"\"\"\n",
    "    def _fn(death_rate, contacts_per_day, days_to_recover, case_fatality_rate_multiplier):\n",
    "        infected = 0.04 * starting_cases * case_fatality_rate_multiplier # Starting population infected = case fatality rate\n",
    "        t, s, i, r = compute_sir(\n",
    "            sampling_rate,\n",
    "            len(deaths),\n",
    "            population * death_rate,\n",
    "            infected,\n",
    "            contacts_per_day,\n",
    "            days_to_recover\n",
    "        )\n",
    "        mse = np.mean(np.square(data - r[::sampling_rate]))\n",
    "        return mse\n",
    "    return _fn\n",
    "\n",
    "\n",
    "def plot_sir_model(r, i, total_days, df, sampling_rate, name):\n",
    "    \"\"\"Plot the model death rates and total deaths vs actual data.\n",
    "    \n",
    "    Args:\n",
    "        r: Array holding recovery values\n",
    "        df: Dataframe holding death values.\n",
    "        sampling_rate: Number of samples per day used to simulate the model.\n",
    "    \"\"\"\n",
    "    plot_start_time = df['Date'].min().timestamp()\n",
    "    plot_step_size = 24 * 60 * 60 / sampling_rate\n",
    "    plot_end_time = plot_start_time + total_days * 24 * 60 * 60 \n",
    "    plot_timestamps = np.arange(plot_start_time, plot_end_time, plot_step_size)\n",
    "    plot_dates = [datetime.utcfromtimestamp(x) for x in plot_timestamps]\n",
    "    print('peak date', plot_dates[np.argmax(i)])\n",
    "    # Plot peak infection\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    ax.ticklabel_format(style='plain')\n",
    "    ax.plot(plot_dates[:-sampling_rate],\n",
    "            (r[sampling_rate:] - r[:-sampling_rate]),\n",
    "            c='g',\n",
    "            label='model death rate',\n",
    "            linewidth=4)\n",
    "    ax.plot(df['Date'].to_list()[:-1],\n",
    "            (df['Deaths'] - df['Deaths'].shift())[1:], label='actual death rate', c='r', linewidth=4)\n",
    "    ax.set_title('SIR model for ' + name)\n",
    "    ax.set_xlabel('Number of days')\n",
    "    ax.set_ylabel('Number of individuals')\n",
    "    plt.legend()\n",
    "    plt.plot()\n",
    "    \n",
    "    # Plot recovery\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    ax.ticklabel_format(style='plain')\n",
    "    ax.plot(plot_dates, r, c='g',\n",
    "            label='model deaths', linewidth=4)\n",
    "    ax.plot(df['Date'].to_list(), df['Deaths'], label='actual deaths', c='r', linewidth=4)\n",
    "    ax.set_title('SIR model for ' + name)\n",
    "    ax.set_xlabel('Number of days')\n",
    "    ax.set_ylabel('Number of individuals')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_time_series_for_area(area_of_interest_spec):\n",
    "    \"\"\"Slice the dataframe by the are of interest and aggregate confirmed cases and deaths by date.\n",
    "    \n",
    "    NOTE: this function only works if you have already prefetched all of the data in the above cell!\n",
    "    \n",
    "    Args:\n",
    "      area_of_interest_spec: A tuple (country, area_name, state_fips, county_fips).\n",
    "    \n",
    "    Returns:\n",
    "      A Dataframe holding a time series of confirmed cases and deaths for the area of interest.\n",
    "    \"\"\"\n",
    "    country = area_of_interest_spec[0]\n",
    "    title = area_of_interest_spec[1]\n",
    "    if country == 'US':\n",
    "        state_fips = area_of_interest_spec[2]\n",
    "        county_fips = area_of_interest_spec[3]\n",
    "\n",
    "        if not county_fips:\n",
    "            population = county_census_df[(county_census_df.STATE == state_fips)\n",
    "                                          & (county_census_df.COUNTY == 0)]['POPESTIMATE2019'].sum()\n",
    "            area_df = us_combined_df[\n",
    "                (us_combined_df.FIPS > state_fips * 1000)\n",
    "                & (us_combined_df.FIPS < (state_fips + 1) * 1000)].groupby('Date').agg(\n",
    "                {'Cases': 'sum',\n",
    "                 'Deaths': 'sum',\n",
    "                })\n",
    "        else:\n",
    "            combined_fips = [state_fips * 1000 + y for y in county_fips]\n",
    "            population = county_census_df[(county_census_df.STATE == state_fips)\n",
    "                                          & (county_census_df.COUNTY.isin(county_fips))]['POPESTIMATE2019'].sum()\n",
    "            area_df = us_combined_df[(us_combined_df.FIPS.isin(combined_fips))].groupby('Date').agg({\n",
    "                'Cases': 'sum',\n",
    "                'Deaths': 'sum',\n",
    "            })\n",
    "        area_df = area_df.reset_index()\n",
    "        area_df = area_df.drop_duplicates(['Date'], keep='last')\n",
    "    else:\n",
    "        area_df = world_combined_df[(world_combined_df.Country_Region == country)]\n",
    "        area_df = area_df.drop_duplicates(['Date', 'Country_Region'], keep='last')\n",
    "        population = area_df['Population'].iloc[0]\n",
    "    print('Total population', population)\n",
    "    return area_df, population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIR model for large counties\n",
    "\n",
    "A simple [SIR model](https://mathworld.wolfram.com/Kermack-McKendrickModel.html) can be used to model the spread of infections across a population. The SIR model maintains 3 different population subgroups: \n",
    "\n",
    "* **(S)usceptible**: Number of individuals who are susceptible to getting the infection.\n",
    "* **(I)nfected**: Number of individuals currently with the disease (or currently contagious).\n",
    "* **(R)ecovered**: Number of individuals who have recovered from the disease and are immune and no longer contagious.<sup>+</sup>\n",
    "\n",
    "The differential equations guiding the SIR process over time (t) is:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\frac{dS}{dt} = -\\beta SI \\\\\n",
    "\\frac{dI}{dt} = \\beta SI - \\gamma I \\\\\n",
    "\\frac{dR}{dt} = \\gamma I \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "One way to conceptualize the SIR model is a set of 3 buckets representing the populations. $\\frac{dS}{dt}$ is the rate of growth in S, which should be negative since the population will continue to get infected. $-\\beta SI$ represents the rate at which people get infected. If you imagine S susceptible individuals each mingling randomly with I infected individuals, this is why $SI$ exists. Beta represents the rate of disease transmission between individuals, i.e. the chance that the disease is transmitted between a random susceptible person to a random infected person. $\\beta SI$ term exists in the second line because this is the negative of $\\frac{dS}{dt}$, i.e. the rate of susceptible population getting infected. Additionally, $-\\gamma I$, where $\\gamma$ is the recovery rate, indicates that infected individuals will gradually lessen as they recover (or die) and become part of the recovered group. $\\frac{dR}{dt}$ is simply the recovery rate.\n",
    "\n",
    "\n",
    "## Model Simulation Details\n",
    "\n",
    "We use first order Taylor expansion to simulate the process using approximations at every fractional time step. Note that higher order terms can be derived by applying the chain rule and substituting in the differential equations above, but with time steps much smaller than contact and recovery times, a first order approximator is probably sufficient for an accurate model.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "S(t+\\delta) &= S(t) &- \\beta S(t)I(t)\\delta &+ ...\\\\\n",
    "I(t+\\delta) &= I(t) &+ (\\beta S(t) - \\gamma)I(t)\\delta &+ ...\\\\\n",
    "R(t+\\delta) &= R(t) &+ \\gamma I(t)\\delta &+... \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "<sup>+</sup> Because death rates are very low compared to the overall population, we do not need to consider a 4th metric (i.e. death) here, but essentially deaths could be modeled as a fraction of the \"recovered\" bucket as they are no longer contagious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out a model plot\n",
    "sampling_rate = 10.0 # Times per day\n",
    "total_days = 180 # Total number of days\n",
    "population = 327.2 # Population in the US in millions\n",
    "infected = 0.04 # Starting population dead = cases * case fatality rate\n",
    "contacts_per_day = 0.2 # Average contacts per day for people\n",
    "days_to_recover = 24 # Average days to recover (Avg 10 days until symptoms + 14 days after symptoms fade)\n",
    "\n",
    "t, s, i, r = compute_sir(sampling_rate, total_days, population, infected, contacts_per_day, days_to_recover)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "ax.ticklabel_format(style='plain')\n",
    "ax.plot(t, s, c='y', label='susceptible', linewidth=4)\n",
    "ax.plot(t, i, c='r', label='infected', linewidth=4)\n",
    "ax.plot(t, r, c='g', label='recovered', linewidth=4)\n",
    "ax.set_title('SIR model')\n",
    "ax.set_xlabel('Number of days')\n",
    "ax.set_ylabel('Number of individuals (in millions)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on looing up state and county FIPS\n",
    "\n",
    "lookup_df = county_census_df[(county_census_df.STNAME == 'Illinois') &\n",
    "                             (county_census_df.CTYNAME.isin([\n",
    "                                 'Cook County',\n",
    "                                 'DeKalb County',\n",
    "                                 'DuPage County',\n",
    "                                 'Grundy County',\n",
    "                                 'Kankakee County',\n",
    "                                 'Kane County',\n",
    "                                 'Kendall County',\n",
    "                                 'McHenry County',\n",
    "                                 'Will County',\n",
    "                             ]))]\n",
    "print(lookup_df['STATE'].iloc[0], ',', lookup_df['COUNTY'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATION_DAYS = 120 # Total number of days to simulate when plotting forecast model.\n",
    "SAMPLING_RATE = 10 # Modeling time samples per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions\n",
    "\n",
    "Some interesting areas (Name, State FIPS, County FIPS) below. Copy one of the values in the bullet points into AREA_OF_INTEREST below.\n",
    "* ('US', 'NYC', 36, [5, 47, 61, 81, 85])\n",
    "* ('US', 'New Orleans', 22, [51, 71, 75, 87, 89, 95, 103, 105])\n",
    "* ('US', 'Detroit', 26, [87, 93, 99, 125, 147, 163])\n",
    "* ('US', 'Bay Area, CA', 6, [1, 13, 41, 55, 75, 81, 85, 95, 97])\n",
    "* ('US', 'Greater LA Area, CA', 6, [37, 59, 65, 71, 111])\n",
    "* ('US', 'Chicago', 17, [31, 37, 43, 63, 89, 91, 93, 111, 197])\n",
    "\n",
    "If County FIPS is empty, this will fetch stats for the whole state:\n",
    "* ('US', 'California', 6, [])\n",
    "* ('US', 'New York', 36, [])\n",
    "* ('US', 'Washington', 53, [])\n",
    "\n",
    "If Country is not US, this will fetch a country's total stats:\n",
    "* ('Italy', 'Italy')\n",
    "* ('Spain', 'Spain')\n",
    "* ('Germany', 'Germany') : A rapid recovery in Germany will fool the model because it's expecting the same rate of infection throughout.\n",
    "* ('France', 'France')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AREA_OF_INTEREST = ('Italy', 'Italy')\n",
    "#AREA_OF_INTEREST = ('US', 'Washington', 53, [])\n",
    "#AREA_OF_INTEREST = ('US', 'NYC', 36, [5, 47, 61, 81, 85])\n",
    "MODEL_FIT_LAST_DATE = '2020-04-08'  # Fit model to data before this date, reserving later dates as holdout.\n",
    "\n",
    "area_df, population = get_time_series_for_area(AREA_OF_INTEREST)\n",
    "area_df = area_df[area_df.Date <= MODEL_FIT_LAST_DATE]\n",
    "# Validate selection through plot and inspection\n",
    "plt.plot(area_df['Date'], area_df['Deaths'])\n",
    "area_df.tail() # Check last entries (Make sure data is good first!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Values to the Model\n",
    "\n",
    "We try to find the best fit of all parameters of the model by minimizing its mean squared error (mse) from actual data points.\n",
    "\n",
    "Note that the simple algorithm used below is randomized and not guaranteed to be optimal, but in practice, seems to converge to a near optimal solution quickly. Also, approaches such as Bayesian optimization, annealing, and other guaranteed optimal techniques take a long time to run per iteration and have occasionally stalled the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial random exploration\n",
    "\n",
    "INIT_RANDOM_PTS = 10000 # Randomly sample this many points before starting optimization\n",
    "\n",
    "# Reasonable search regions for each parameter\n",
    "initial_death_rate_interval = (0.0001, 0.02)\n",
    "initial_contacts_per_day_interval = (0.05, 2.0)\n",
    "initial_days_to_recover_interval = (5.0, 50.0)\n",
    "initial_case_fatality_multiplier_interval = (0.05, 100.0)\n",
    "\n",
    "data = area_df['Deaths'].to_numpy()\n",
    "starting_cases = area_df['Cases'].min()\n",
    "\n",
    "mse_metric = create_mse_objective_fn(data, population, SAMPLING_RATE, starting_cases)\n",
    "\n",
    "\n",
    "min_bounds = np.array([initial_death_rate_interval[0],\n",
    "                       initial_contacts_per_day_interval[0],\n",
    "                       initial_days_to_recover_interval[0],\n",
    "                       initial_case_fatality_multiplier_interval[0],\n",
    "                      ])\n",
    "max_bounds = np.array([initial_death_rate_interval[1],\n",
    "                       initial_contacts_per_day_interval[1],\n",
    "                       initial_days_to_recover_interval[1],\n",
    "                       initial_case_fatality_multiplier_interval[1],\n",
    "                      ])\n",
    "\n",
    "death_rate_interval = initial_death_rate_interval\n",
    "\n",
    "# Initially randomly sample a large number of points\n",
    "params = np.random.uniform(min_bounds, max_bounds, size=(INIT_RANDOM_PTS, 4))\n",
    "\n",
    "values = [mse_metric(row[0], row[1], row[2], row[3]) for row in params]\n",
    "print('MSE', np.min(values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterations involve batch random sampling at local and globally bounded levels.\n",
    "\n",
    "ITERATIONS = 25 # Each iteration may take several seconds.\n",
    "EXPLORATION_PTS = 2000 # Run this many random explorations per step\n",
    "LOCAL_SEARCH_PER_TOP_K = 100 # Run this many local explorations per candidate point\n",
    "TOP_K = 20 # Maintain top K optimal points as local search areas and bounding box for new search region\n",
    "reg = 0.1 # Regularization: stretch the boundaries of the bounding box by this fraction every iteration for exploration\n",
    "local_search_range_multiplier = 0.001 # This fraction of the total boundary length is used for local search\n",
    "\n",
    "for iters in range(ITERATIONS):\n",
    "\n",
    "    top_k_indices = np.argsort(values)[:TOP_K]\n",
    "    top_k_params = params[top_k_indices]\n",
    "    top_k_values = [values[k] for k in top_k_indices]\n",
    "    top_value = np.min(values)\n",
    "    print('Iteration', iters)\n",
    "    print('Best MSE so far', top_value)\n",
    "\n",
    "    # Compute unbounded top_k bounding box to get range of expansion of bounding box.\n",
    "    top_k_min_bounds = np.min(top_k_params, axis=0)\n",
    "    top_k_max_bounds = np.max(top_k_params, axis=0)\n",
    "    top_k_range = (top_k_max_bounds - top_k_min_bounds) / 2\n",
    "    # Bound expanded bounding box by the initial bounding box so box will not go out of maximum search range.\n",
    "    top_k_min_bounds = np.maximum(top_k_min_bounds - top_k_range * reg, min_bounds)\n",
    "    top_k_max_bounds = np.minimum(top_k_max_bounds + top_k_range * reg, max_bounds)\n",
    "    top_k_range = (top_k_max_bounds - top_k_min_bounds) / 2\n",
    "    print('New death_rate_interval', top_k_min_bounds[0], top_k_max_bounds[0])\n",
    "    print('New contacts_per_day_interval', top_k_min_bounds[1], top_k_max_bounds[1])\n",
    "    print('New days_to_recover_interval', top_k_min_bounds[2], top_k_max_bounds[2])\n",
    "    print('New case_fatality_multiplier_interval', top_k_min_bounds[3], top_k_max_bounds[3])\n",
    "\n",
    "    # Do some local search\n",
    "    for i in range(TOP_K):\n",
    "        tmp_params = top_k_params[i]\n",
    "        local_min_bounds = np.maximum(min_bounds, tmp_params - top_k_range)\n",
    "        local_max_bounds = np.minimum(max_bounds, tmp_params + top_k_range)\n",
    "        local_params = np.random.uniform(\n",
    "            local_min_bounds, local_max_bounds, size=(LOCAL_SEARCH_PER_TOP_K, 4)\n",
    "        )\n",
    "        local_values = [mse_metric(row[0], row[1], row[2], row[3]) for row in local_params]\n",
    "        best_local_value = np.min(local_values)\n",
    "        if best_local_value < top_k_values[i]:\n",
    "            if best_local_value < top_value:\n",
    "                print('found new min {} in {}(th) best neighborhood'.format(best_local_value, len(top_k_values) - i))\n",
    "            params[top_k_indices[i]] = local_params[np.argmin(local_values)]\n",
    "            values[top_k_indices[i]] = best_local_value\n",
    "\n",
    "    # Do some top-bounded exploration\n",
    "    new_params = np.random.uniform(top_k_min_bounds, top_k_max_bounds, size=(EXPLORATION_PTS, 4))\n",
    "    params = np.concatenate((params, new_params))\n",
    "    values.extend([mse_metric(row[0], row[1], row[2], row[3]) for row in new_params])\n",
    "    print('Total points explored', params.shape[0])\n",
    "    print('MSE', np.min(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Validation plot\n",
    "validation_area_df, _ = get_time_series_for_area(AREA_OF_INTEREST)\n",
    "\n",
    "best_index = np.argmin(values)\n",
    "best_params = params[best_index]\n",
    "best_death_rate = best_params[0]\n",
    "best_contacts = best_params[1]\n",
    "best_days_to_recover = best_params[2]\n",
    "best_case_fatality_multiplier = best_params[3]\n",
    "best_mse = np.min(values)\n",
    "\n",
    "infected = 0.04 * starting_cases * best_case_fatality_multiplier\n",
    "t, s, i, r = compute_sir(\n",
    "    SAMPLING_RATE,\n",
    "    SIMULATION_DAYS,\n",
    "    population * best_death_rate,\n",
    "    infected,\n",
    "    best_contacts,\n",
    "    best_days_to_recover\n",
    ")\n",
    "\n",
    "print('best_death_rate:', best_death_rate)\n",
    "print('best_contact:', best_contacts)\n",
    "print('best_days_to_recover:', best_days_to_recover)\n",
    "print('best_case_fatality_multiplier', best_case_fatality_multiplier)\n",
    "print('best_mse', best_mse)\n",
    "plot_sir_model(r, i, SIMULATION_DAYS, validation_area_df, SAMPLING_RATE, AREA_OF_INTEREST[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_metric(0.0040, 0.35338, 24.0647, 0.1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
