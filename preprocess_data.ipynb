{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "Remember to run fetch_data.sh first to download other required github data repos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def get_county(string):\n",
    "    str_array = string.split(',')\n",
    "    if len(str_array) < 3:\n",
    "        return ''\n",
    "    else:\n",
    "        return str_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that reference data directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the reference directory exists\n",
    "for dirname, _, filenames in os.walk('../COVID-19/csse_covid_19_data/csse_covid_19_time_series/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read covid 19 and census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covid confirmed cases and deaths\n",
    "us_confirmed_df = pd.read_csv('../COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')\n",
    "us_deaths_df = pd.read_csv('../COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv')\n",
    "world_confirmed_df = pd.read_csv('../COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "world_deaths_df = pd.read_csv('../COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "\n",
    "# US County level census data\n",
    "# From https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv\n",
    "county_census_df = pd.read_csv('./data/co-est2019-alldata.csv', encoding='latin-1')\n",
    "county_census_df.head()\n",
    "\n",
    "# World population data (for MOST countries)\n",
    "world_population_df = pd.read_csv('./data/country_profile_variables.csv')\n",
    "world_population_df = world_population_df[['country', 'Population in thousands (2017)']]\n",
    "world_population_df = world_population_df.rename(columns={'Population in thousands (2017)': 'Population'})\n",
    "# Add some missing countries\n",
    "world_population_df.loc[len(world_population_df)] = ['Taiwan*', 23780]\n",
    "world_population_df['Population'] *= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine US County COVID and Census Data\n",
    "\n",
    "The output will be written into your ./data directory as `us_combined_df.pkl`. If this file already exists, the following cell will do an incremental update on the most recent missing dates.\n",
    "\n",
    "If you suspect having corrupted data, you can delete this file, and the following cell will recreate the dataframe from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined dataframe if it already exists, and select date to do incremental updates.\n",
    "\n",
    "# Output dataframe columns\n",
    "column_names = ['FIPS', 'County', 'Province_State', 'Country_Region', 'Date', 'Cases', 'Deaths', 'Population']\n",
    "\n",
    "if os.path.exists('./data/us_combined_df.pkl'):\n",
    "    with open('./data/us_combined_df.pkl', 'rb') as f:\n",
    "        us_combined_df = pickle.load(f)\n",
    "else:\n",
    "    us_combined_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "if len(us_combined_df):\n",
    "    start_date = us_combined_df['Date'].max() + timedelta(0, 0, 1)\n",
    "else:\n",
    "    start_date = datetime(2019, 1, 1)\n",
    "\n",
    "# Use date conversions to extract correct dates from column names in COVID dataset\n",
    "date_cols = [x for x in list(us_confirmed_df) if is_date(x)]\n",
    "dates = [datetime.strptime(x , '%m/%d/%y') for x in date_cols]\n",
    "dates = [x for x in dates if x >= start_date]\n",
    "date_cols = [x for x in date_cols if datetime.strptime(x, '%m/%d/%y') >= start_date]\n",
    "\n",
    "# Append rows that have confirmed cases, deaths, and populations included.\n",
    "for index, row in us_confirmed_df.iterrows():\n",
    "    fips = row['FIPS']\n",
    "    county = get_county(row['Combined_Key'])\n",
    "    if math.isnan(fips):\n",
    "        print('skipping county', county, fips, population)\n",
    "        continue\n",
    "    population = county_census_df[\n",
    "        (county_census_df.STATE == int(fips / 1000))\n",
    "        & (county_census_df.COUNTY == int(fips % 1000))]['POPESTIMATE2019']\n",
    "    if len(population) != 1:\n",
    "        print('skipping county', county, fips, population)\n",
    "        continue\n",
    "    population = population.to_numpy()[0]\n",
    "    for (date_col, date) in zip(date_cols, dates):\n",
    "        confirmed = row[date_col]\n",
    "        if confirmed == 0:\n",
    "            continue\n",
    "        if date_col in us_deaths_df:\n",
    "            deaths = us_deaths_df[us_deaths_df.FIPS == row['FIPS']][date_col].to_numpy()[0]\n",
    "        else:\n",
    "            deaths = 0\n",
    "            \n",
    "        values = [fips, county, row['Province_State'], row['Country_Region'], date, confirmed, deaths, population]\n",
    "        df_length = len(us_combined_df)\n",
    "        us_combined_df.loc[df_length] = values\n",
    "    if index % 100 == 0:\n",
    "        print('processed {} out of {}'.format(index, len(us_confirmed_df)))\n",
    "us_combined_df = us_combined_df.drop_duplicates(['Date', 'FIPS'], keep='last') # Drop duplicates just in case\n",
    "us_combined_df.tail() # Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to disk\n",
    "with open('./data/us_combined_df.pkl', 'wb') as f:\n",
    "    pickle.dump(us_combined_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine worldwide country population and COVID-19 data\n",
    "\n",
    "The output will be written into your ./data directory as `world_combined_df.pkl`. If this file already exists, the following cell will do an incremental update on the most recent missing dates.\n",
    "\n",
    "If you suspect having corrupted data, you can delete this file, and the following cell will recreate the dataframe from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Country_Region', 'Date', 'Cases', 'Deaths', 'Population']\n",
    "\n",
    "if os.path.exists('./data/world_combined_df.pkl'):\n",
    "    with open('./data/world_combined_df.pkl', 'rb') as f:\n",
    "        world_combined_df = pickle.load(f)\n",
    "else:\n",
    "    world_combined_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "if len(world_combined_df):\n",
    "    start_date = world_combined_df['Date'].max() + timedelta(0, 0, 1)\n",
    "else:\n",
    "    start_date = datetime(2019, 1, 1)\n",
    "\n",
    "# Use date conversions to extract correct dates from column names in COVID dataset\n",
    "date_cols = [x for x in list(us_confirmed_df) if is_date(x)]\n",
    "dates = [datetime.strptime(x , '%m/%d/%y') for x in date_cols]\n",
    "dates = [x for x in dates if x >= start_date]\n",
    "date_cols = [x for x in date_cols if datetime.strptime(x, '%m/%d/%y') >= start_date]\n",
    "\n",
    "# Aggregate regions for countries\n",
    "world_confirmed_agg_df = world_confirmed_df.groupby(['Country/Region']).agg({\n",
    "    d: 'sum' for d in date_cols\n",
    "}).reset_index()\n",
    "world_deaths_agg_df = world_deaths_df.groupby(['Country/Region']).agg({\n",
    "    d: 'sum' for d in date_cols\n",
    "}).reset_index()\n",
    "\n",
    "# Start populating combined dataframe to pickle\n",
    "for index, row in world_confirmed_agg_df.iterrows():\n",
    "    country = row['Country/Region']\n",
    "    population = world_population_df[world_population_df['country'] == row['Country/Region']]['Population']\n",
    "    if len(population) != 1:\n",
    "        print('skipping country', country, population)\n",
    "        continue\n",
    "    population = population.to_numpy()[0]\n",
    "    for (date_col, date) in zip(date_cols, dates):\n",
    "        confirmed = row[date_col]\n",
    "        if confirmed == 0:\n",
    "            continue\n",
    "        if date_col in world_deaths_agg_df:\n",
    "            deaths = world_deaths_agg_df[\n",
    "                (world_deaths_agg_df['Country/Region'] == row['Country/Region'])\n",
    "            ][date_col].to_numpy()[0]\n",
    "        else:\n",
    "            deaths = 0\n",
    "            \n",
    "        values = [country, date, confirmed, deaths, population]\n",
    "        df_length = len(world_combined_df)\n",
    "        world_combined_df.loc[df_length] = values\n",
    "    if index % 100 == 0:\n",
    "        print('processed {} out of {}'.format(index, len(world_confirmed_agg_df)))\n",
    "world_combined_df.tail()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/world_combined_df.pkl', 'wb') as f:\n",
    "    pickle.dump(world_combined_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
